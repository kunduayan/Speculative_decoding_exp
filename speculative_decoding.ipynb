{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f201b26a0144c0dba37dcb95f419587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b9235bf7633447fa71f969991e596f7",
              "IPY_MODEL_c7729c9826fe4566afb5d3c419a52a51",
              "IPY_MODEL_16e9f10c4ea04a6b84e3b80cc606e802"
            ],
            "layout": "IPY_MODEL_4bbaae821697476ea35bb645ae4ddb28"
          }
        },
        "4b9235bf7633447fa71f969991e596f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c02d3fb8e8034ebf9db7b8c40f3152a8",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c153714fbb4d9d87e5215e64d776df",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c7729c9826fe4566afb5d3c419a52a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d5c8488f624241b0b32449fe976985",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdc9aee85c854883bde368ba0b166a7f",
            "value": 2
          }
        },
        "16e9f10c4ea04a6b84e3b80cc606e802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa7d7a6f56714fe7be0347010ad084d9",
            "placeholder": "​",
            "style": "IPY_MODEL_a86d98a453624774898e41707472a067",
            "value": " 2/2 [00:00&lt;00:00,  2.63it/s]"
          }
        },
        "4bbaae821697476ea35bb645ae4ddb28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02d3fb8e8034ebf9db7b8c40f3152a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c153714fbb4d9d87e5215e64d776df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56d5c8488f624241b0b32449fe976985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc9aee85c854883bde368ba0b166a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa7d7a6f56714fe7be0347010ad084d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a86d98a453624774898e41707472a067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MrGBxocbATc_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"/content/Speculative-Decoding-main/\")"
      ],
      "metadata": {
        "id": "SqbuFRZyuG6S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "ryffpqS4uJPH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "# We will use the Google Llama-3.2 3B Instruct as the model we want to accelerate (3B parameters)\n",
        "target_model_name = \"google/flan-t5-xl\"\n",
        "target = AutoModelForSeq2SeqLM.from_pretrained(target_model_name)\n",
        "target.to(device)\n",
        "\n",
        "# We will use the Google Llama-3.2 1B Instruct as the drafter model (1B parameters)\n",
        "drafter_model_name = \"google/flan-t5-small\"\n",
        "drafter = AutoModelForSeq2SeqLM.from_pretrained(drafter_model_name)\n",
        "drafter.to(device)\n",
        "\n",
        "# Don't forget to load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "1f201b26a0144c0dba37dcb95f419587",
            "4b9235bf7633447fa71f969991e596f7",
            "c7729c9826fe4566afb5d3c419a52a51",
            "16e9f10c4ea04a6b84e3b80cc606e802",
            "4bbaae821697476ea35bb645ae4ddb28",
            "c02d3fb8e8034ebf9db7b8c40f3152a8",
            "c0c153714fbb4d9d87e5215e64d776df",
            "56d5c8488f624241b0b32449fe976985",
            "cdc9aee85c854883bde368ba0b166a7f",
            "aa7d7a6f56714fe7be0347010ad084d9",
            "a86d98a453624774898e41707472a067"
          ]
        },
        "id": "EhTy2eHYuLNj",
        "outputId": "19ad4682-64ab-47e7-8380-869ae6d9f3ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f201b26a0144c0dba37dcb95f419587"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Translate to English: Je m'appelle Ayan. N'hésitez pas à contribuer à mon projet !\"\n",
        "# prefix = \"A group of flamingos is called \"\n",
        "# chat_templated = f\"<bos><start_of_turn>user\\n{prefix}<end_of_turn>\\n<start_of_turn>model\\n\" # Gemma chat template\n",
        "chat_templated = prefix\n",
        "input_ids = tokenizer(chat_templated, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids[0].tolist() # Generation methods require a list of ids"
      ],
      "metadata": {
        "id": "TqY7hBCruNls"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sampling import speculative_generate, autoregressive_generate\n",
        "from sampling import speculative_generate_encoder_decoder, autoregressive_generate_encoder_decoder\n",
        "from utils.logits_processor import NucleusProcessor\n",
        "\n",
        "# Parameters\n",
        "gen_len = 100       # Maximum number of tokens generated (could over pass when using speculative decoding)\n",
        "gamma = [3, 5, 7]  # Number of drafts generated by the drafter model at each step\n",
        "logits_processor = NucleusProcessor(temperature=.6, top_p=.5) # Nucleus sampling with p=0.9 and T=0.6\n",
        "\n",
        "\n",
        "\n",
        "# Number of drafts accepted by the target model divided by the number of drafts generated"
      ],
      "metadata": {
        "id": "FLPpxNEJuRl-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# Generate text using the classic auto-regressive decoding (slow)\n",
        "output_ids_ar = autoregressive_generate_encoder_decoder( # or autoregressive_generate_encoder_decoder for encoder-decoder models\n",
        "                input_ids,\n",
        "                target,\n",
        "                logits_processor=logits_processor,\n",
        "                max_gen_len=gen_len,\n",
        "                eos_tokens_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "output_ar = tokenizer.decode(output_ids_ar, skip_special_tokens=True)\n",
        "end_time = time.time()\n",
        "print(\"Time taken: \", end_time - start_time)\n",
        "output_ar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4FTcttgp1fd0",
        "outputId": "54ae8a13-900f-4677-c063-0379113402a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken:  3.5887839794158936\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm Ayan. Please don't hesitate to contribute to my project!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in gamma:\n",
        "  print(\"gamma value: \", i)\n",
        "  start_time = time.time()\n",
        "  # Generate text using the speculative decoding (faster)\n",
        "  output_ids_sd, alpha = speculative_generate_encoder_decoder( # or speculative_generate_encoder_decoder for encoder-decoder models\n",
        "                  input_ids,\n",
        "                  drafter,\n",
        "                  target,\n",
        "                  logits_processor=logits_processor,\n",
        "                  gamma=i,\n",
        "                  max_gen_len=gen_len,\n",
        "                  eos_tokens_id=tokenizer.eos_token_id,\n",
        "                  pad_token_id=tokenizer.pad_token_id,\n",
        "              )\n",
        "  output_sd = tokenizer.decode(output_ids_sd, skip_special_tokens=True)\n",
        "  end_time = time.time()\n",
        "  print(\"Time taken: \", end_time - start_time)\n",
        "  print(\"alpha: \", alpha)\n",
        "  print(\"Generated output: \", output_sd)\n",
        "  print(\"######\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq7oK9Yr1hnO",
        "outputId": "bb252c62-e64b-4340-b361-3a219773f025"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gamma value:  3\n",
            "Time taken:  2.0771307945251465\n",
            "alpha:  0.6666666666666666\n",
            "Generated output:  I'm Ayan. Don't hesitate to contribute to my project!\n",
            "######\n",
            "gamma value:  5\n",
            "Time taken:  0.864091157913208\n",
            "alpha:  0.7\n",
            "Generated output:  My name is Ayan. Don't hesitate to contribute to my project!\n",
            "######\n",
            "gamma value:  7\n",
            "Time taken:  1.5532920360565186\n",
            "alpha:  0.2857142857142857\n",
            "Generated output:  I am Ayan. Don't hesitate to contribute to my project !\n",
            "######\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 3\n",
        "\n",
        "output_ids_sd, alpha = speculative_generate_encoder_decoder( # or speculative_generate_encoder_decoder for encoder-decoder models\n",
        "                  input_ids,\n",
        "                  drafter,\n",
        "                  target,\n",
        "                  logits_processor=logits_processor,\n",
        "                  gamma=gamma,\n",
        "                  max_gen_len=gen_len,\n",
        "                  eos_tokens_id=tokenizer.eos_token_id,\n",
        "                  pad_token_id=tokenizer.pad_token_id,\n",
        "              )\n",
        "output_sd = tokenizer.decode(output_ids_sd, skip_special_tokens=True)\n",
        "print(\"Generated output: \", output_sd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8bfin51MlIL",
        "outputId": "196d42ff-eeee-47d7-b852-ffc7b5a5ba5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Draft model input: tensor([[ 0, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
            "Draft model output:  [0, 27, 580, 71, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Target model input:  tensor([[  0,  27, 580,  71,  63]], device='cuda:0')\n",
            "target model output: tensor([[31]], device='cuda:0')\n",
            "target model output: tensor([[1512]], device='cuda:0')\n",
            "target model output: tensor([[63]], device='cuda:0')\n",
            "Draft model input: tensor([[ 0, 27, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
            "Draft model output:  [0, 27, 31, 51, 3874, 71, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Target model input:  tensor([[   0,   27,   31,   51, 3874,   71]], device='cuda:0')\n",
            "target model output: tensor([[51]], device='cuda:0')\n",
            "target model output: tensor([[71]], device='cuda:0')\n",
            "target model output: tensor([[1512]], device='cuda:0')\n",
            "Draft model input: tensor([[ 0, 27, 31, 51, 71,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
            "Draft model output:  [0, 27, 31, 51, 71, 63, 152, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Target model input:  tensor([[  0,  27,  31,  51,  71,  63, 152,   5]], device='cuda:0')\n",
            "target model output: tensor([[63]], device='cuda:0')\n",
            "target model output: tensor([[152]], device='cuda:0')\n",
            "target model output: tensor([[5]], device='cuda:0')\n",
            "Draft model input: tensor([[  0,  27,  31,  51,  71,  63, 152,   5, 863,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0]], device='cuda:0')\n",
            "Draft model output:  [0, 27, 31, 51, 71, 63, 152, 5, 863, 278, 31, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Target model input:  tensor([[  0,  27,  31,  51,  71,  63, 152,   5, 863, 278,  31,  17]],\n",
            "       device='cuda:0')\n",
            "target model output: tensor([[278]], device='cuda:0')\n",
            "target model output: tensor([[31]], device='cuda:0')\n",
            "target model output: tensor([[17]], device='cuda:0')\n",
            "Draft model input: tensor([[    0,    27,    31,    51,    71,    63,   152,     5,   863,   278,\n",
            "            31,    17, 10888,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0]], device='cuda:0')\n",
            "Draft model output:  [0, 27, 31, 51, 71, 63, 152, 5, 863, 278, 31, 17, 10888, 12, 4139, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Target model input:  tensor([[    0,    27,    31,    51,    71,    63,   152,     5,   863,   278,\n",
            "            31,    17, 10888,    12,  4139,    12]], device='cuda:0')\n",
            "target model output: tensor([[12]], device='cuda:0')\n",
            "target model output: tensor([[4139]], device='cuda:0')\n",
            "target model output: tensor([[12]], device='cuda:0')\n",
            "Draft model input: tensor([[    0,    27,    31,    51,    71,    63,   152,     5,   863,   278,\n",
            "            31,    17, 10888,    12,  4139,    12,    82,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0]], device='cuda:0')\n",
            "Draft model output:  [0, 27, 31, 51, 71, 63, 152, 5, 863, 278, 31, 17, 10888, 12, 4139, 12, 82, 516, 55, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Target model input:  tensor([[    0,    27,    31,    51,    71,    63,   152,     5,   863,   278,\n",
            "            31,    17, 10888,    12,  4139,    12,    82,   516,    55,     1]],\n",
            "       device='cuda:0')\n",
            "target model output: tensor([[516]], device='cuda:0')\n",
            "target model output: tensor([[55]], device='cuda:0')\n",
            "target model output: tensor([[1]], device='cuda:0')\n",
            "draft spculated: 18.0\n",
            "draft accepted: 13.0\n",
            "\n",
            "\n",
            "Generated output:  I'm Ayan. Please don't hesitate to contribute to my project!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHDq-m4tMlRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvfWNMOv2BmP"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}